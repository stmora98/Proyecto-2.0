# **Reto 2 â€“ TransformaciÃ³n intermedia y anÃ¡lisis exploratorio (Silver) ğŸ”§ğŸ“Š**

## **Objetivo y soluciÃ³n paso a paso ğŸ§­**

### **Objetivo ğŸ¯**
Transformar los datos Bronze y realizar anÃ¡lisis exploratorio en Silver. 

---

## **SoluciÃ³n paso a paso ğŸªœ**

### **Set de score crediticio y set de productos ğŸ§©**

- Crear nuevo Dataflow Gen2 
- Configurar el origen en la tabla Bronze. 
- Aplicar transformaciones intermedias 
- Crear columna de score crediticio 
- Segmentar clientes por perfil crediticio 
- Se identifica el cluster con mayor promedio de score 
- Se filtran los clientes pertenecientes a ese cluster
- Resultado: subconjunto de clientes con perfil crediticio alto 

---

# **Segmentacion de clientes por score crediticio ğŸ§®**

Importar funciones necesarias 

```python
from pyspark.sql.functions import col, when, udf 
from pyspark.sql.types import StringType 
from pyspark.ml.feature import VectorAssembler 
from pyspark.ml.clustering import KMeans 
```


## **Cargar tabla Silver con datos financieros ğŸ§¾**

```python
df_fin = spark.read.table("creditScore_silver") 
```

---

## **Derivar columna score_estimado basada en comportamiento de pago y uso de crÃ©dito ğŸ’³**

```python
df_fin = df_fin.withColumn("score_estimado", 
    when(col("Payment_Behaviour") == "High_spent_Small_value_payments", 650)
    .when(col("Payment_Behaviour") == "Low_spent_Large_value_payments", 750)
    .when(col("Payment_Behaviour") == "High_spent_Large_value_payments", 800)
    .when(col("Payment_Behaviour") == "Low_spent_Small_value_payments", 600)
    .otherwise(620)
)
```

---

## **PenalizaciÃ³n por pagos atrasados â°**

```python
df_fin = df_fin.withColumn("score_estimado", 
    col("score_estimado") - (col("Num_of_Delayed_Payment") * 5)
)
```

---

## **PenalizaciÃ³n por alto uso de crÃ©dito ğŸ“‰**

```python
df_fin = df_fin.withColumn("score_estimado", 
    when(col("Credit_Utilization_Ratio") > 0.8, col("score_estimado") - 20)
    .otherwise(col("score_estimado"))
)
```

---

## **Limitar score entre 300 y 850 âš™ï¸**

```python
df_fin = df_fin.withColumn("score_estimado", 
    when(col("score_estimado") < 300, 300)
    .when(col("score_estimado") > 850, 850)
    .otherwise(col("score_estimado"))
)
```

---

## **Filtrar registros vÃ¡lidos para clustering ğŸ§¹**

```python
df_fin_clean = df_fin.filter(col("score_estimado").isNotNull()) 
```

---

## **Vectorizar columna score_estimado para ML ğŸ¤–**

```python
assembler = VectorAssembler(inputCols=["score_estimado"], outputCol="features") 
df_fin_vec = assembler.transform(df_fin_clean) 
```

---

## **Aplicar KMeans clustering para segmentar clientes ğŸ§ **

```python
kmeans = KMeans(k=3, seed=42) 
model_fin = kmeans.fit(df_fin_vec) 
df_fin_clustered = model_fin.transform(df_fin_vec) 
```

---

## **Etiquetar perfiles crediticios segÃºn promedio de score por cluster ğŸ·ï¸**

```python
cluster_scores = df_fin_clustered.groupBy("prediction") \
    .avg("score_estimado") \
    .orderBy("avg(score_estimado)", ascending=False) \
    .collect()
```

---

## **Crear mapa de etiquetas: Alto, Medio, Bajo ğŸ—ºï¸**

```python
cluster_map = {} 
for i, row in enumerate(cluster_scores): 
    cluster_map[row["prediction"]] = ["Alto", "Medio", "Bajo"][i] 
```

---

## **UDF para asignar etiqueta âš¡**

```python
def map_cluster(pred): 
    return cluster_map.get(pred, "Desconocido") 

map_udf = udf(map_cluster, StringType()) 
df_segmentado = df_fin_clustered.withColumn("perfil_crediticio", map_udf(col("prediction"))) 
```

---

## **Contar clientes por perfil (opcional para validaciÃ³n) ğŸ“Š**

```python
df_segmentado.groupBy("perfil_crediticio").count().orderBy("count", ascending=False).show() 
```

---

## **Filtrar clientes con perfil Alto ğŸ¥‡**

```python
df_gold_fin = df_segmentado.filter(col("perfil_crediticio") == "Alto") 
```

---

## **Guardar tabla Gold con clientes de mejor perfil crediticio ğŸ’¾**

```python
df_gold_fin.write.option("mergeSchema", "true").mode("overwrite").saveAsTable("creditScore_gold") 
```

---




# **SegmentaciÃ³n ML + PromociÃ³n a Gold (Retail por producto) ğŸ›ï¸ğŸ¤–**

---

## **ğŸ“Œ Importar funciones necesarias**

```python
from pyspark.sql.functions import col, when, udf 
from pyspark.sql.types import StringType 
from pyspark.ml.feature import VectorAssembler 
from pyspark.ml.clustering import KMeans 
```

---

## **Cargar tabla Silver con catÃ¡logo de productos retail ğŸ§¾**

```python
df_retail = spark.read.table("productos_silver") 
```

---

## **ğŸ§® Derivar columna valor_comercial = Price Ã— Stock**

```python
df_retail = df_retail.withColumn("valor_comercial", col("Price") * col("Stock")) 
```

---

## **ğŸ§® Derivar columna disponibilidad_binaria**

```python
df_retail = df_retail.withColumn("disponible", 
    when(col("Availability") == "InStock", 1).otherwise(0)
)
```

---

## **ğŸ§¹ Filtrar registros vÃ¡lidos para clustering**

```python
df_retail_clean = df_retail.filter( 
    col("valor_comercial").isNotNull() & col("disponible").isNotNull()
)
```

---

## **ğŸ“Š Vectorizar columnas para ML**

```python
assembler = VectorAssembler(inputCols=["valor_comercial", "disponible"], outputCol="features") 
df_retail_vec = assembler.transform(df_retail_clean) 
```

---

## **ğŸ¤– Aplicar KMeans clustering para segmentar productos**

```python
kmeans = KMeans(k=3, seed=42) 
model_retail = kmeans.fit(df_retail_vec) 
df_retail_clustered = model_retail.transform(df_retail_vec) 
```

---

## **ğŸ·ï¸ Etiquetar productos segÃºn valor comercial promedio por cluster**

```python
cluster_scores = df_retail_clustered.groupBy("prediction") \
    .avg("valor_comercial") \
    .orderBy("avg(valor_comercial)", ascending=False) \
    .collect()
```

---

## **Crear mapa de etiquetas: Valioso, Medio, Bajo ğŸ—ºï¸**

```python
cluster_map = {} 
for i, row in enumerate(cluster_scores): 
    cluster_map[row["prediction"]] = ["Valioso", "Medio", "Bajo"][i] 
```

---

## **UDF para asignar etiqueta âš¡**

```python
def map_cluster(pred): 
    return cluster_map.get(pred, "Desconocido") 

map_udf = udf(map_cluster, StringType()) 
df_segmentado = df_retail_clustered.withColumn("perfil_producto", map_udf(col("prediction"))) 
```

---

## **ğŸ” Conteo por perfil (opcional para validaciÃ³n)**

```python
df_segmentado.groupBy("perfil_producto").count().orderBy("count", ascending=False).show() 
```

---

## **ğŸ¥‡ Filtrar productos valiosos y disponibles**

```python
df_gold_retail = df_segmentado.filter(
    (col("perfil_producto") == "Valioso") & (col("disponible") == 1)
)
```

---

## **ğŸ’¾ Guardar tabla Gold con productos valiosos**

```python
df_gold_retail.write.option("mergeSchema", "true").mode("overwrite").saveAsTable("productos_gold") 
```


